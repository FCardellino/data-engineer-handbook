{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee906429-59d5-47d0-a4bd-4d2cc1097087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, split, broadcast\n",
    "\n",
    "#CREATE AND CONFIGURE SPARK SESSION\n",
    "spark = SparkSession.builder.appName(\"SparkFundamentalsWeekHW\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\")\\\n",
    "        .config(\"spark.driver.memory\", \"4g\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"50\")\\\n",
    "        .getOrCreate()\n",
    "##TASK 1: Disabled automatic broadcast join with spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "#Disabled automatic broadcast join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "##END OF TASK 1\n",
    "\n",
    "##TASK 2: Explicitly broadcast JOINs medals and maps\n",
    "#SAVE DATA INTO VARIABLES \n",
    "medalsSelected = spark.read.option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/home/iceberg/data/medals.csv\")\n",
    "mapsSelected = spark.read.option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/home/iceberg/data/maps.csv\")\n",
    "matchesSelected = spark.read.option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/home/iceberg/data/matches.csv\")\n",
    "matchDetailsSelected = spark.read.option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "medalsMatchesPlayersSelected = spark.read.option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .csv(\"/home/iceberg/data/medals_matches_players.csv\")\n",
    "\n",
    "#CREATE TEMP VIEWS FOR EXPLICIT BRODCAST JOIN\n",
    "medalsSelected.createOrReplaceTempView(\"medals\")\n",
    "mapsSelected.createOrReplaceTempView(\"maps\")\n",
    "matchesSelected.createOrReplaceTempView(\"matches\")\n",
    "medalsMatchesPlayersSelected.createOrReplaceTempView(\"medals_matches_players\")\n",
    "\n",
    "#DO AND SAVE EXPLICIT BROADCAST INTO VARIABLEs\n",
    "explicitBroadcastMedals = medalsMatchesPlayersSelected.alias(\"mmp\")\\\n",
    "                                                          .join(broadcast(medalsSelected).alias(\"me\"),col(\"mmp.medal_id\") == col(\"me.medal_id\"))\\\n",
    "                                                          .select(\"mmp.*\",\"me.classification\")\n",
    "explicitBroadcastMaps = matchesSelected.alias(\"m\")\\\n",
    "                                           .join(broadcast(mapsSelected).alias(\"ma\"),col(\"m.mapid\") == col(\"ma.mapid\"))\\\n",
    "                                           .select(\"m.*\",\"ma.name\")\n",
    "\n",
    "##END OF TASK 2\n",
    "\n",
    "##TASK 3: Bucket join match_details, matches, and medal_matches_players on match_id with 16 buckets\n",
    "\n",
    "#CREATE TABLES WITH BUCKETS AND SAVE BUCKETED DATA\n",
    "#CREATE TABLE medals_matches_players_bucketed\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.medals_matches_players_bucketed\"\"\")\n",
    "bucketedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.medals_matches_players_bucketed (\n",
    "     match_id STRING,\n",
    "     player_gamertag STRING,\n",
    "     medal_id STRING,\n",
    "     count INTEGER,\n",
    "     classification STRING\n",
    " )\n",
    " USING iceberg\n",
    " PARTITIONED BY (bucket(16, match_id));\n",
    " \"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "explicitBroadcastMedals.select(\n",
    "    \"match_id\", \"player_gamertag\", \"medal_id\", \"count\", \"classification\"\n",
    "    )\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.medals_matches_players_bucketed\")\n",
    "\n",
    "#CREATE TABLE matches_maps_bucketed\n",
    "#Get distinct completion dates\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.matches_maps_bucketed\"\"\")\n",
    "bucketedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_maps_bucketed (\n",
    "     match_id STRING,\n",
    "     mapid STRING,\n",
    "     name STRING,\n",
    "     is_team_game BOOLEAN,\n",
    "     playlist_id STRING,\n",
    "     completion_date TIMESTAMP\n",
    " )\n",
    " USING iceberg\n",
    " PARTITIONED BY (completion_date,bucket(16, match_id));\n",
    " \"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "explicitBroadcastMaps.select(\n",
    "    \"match_id\", \"mapid\", \"name\", \"is_team_game\", \"playlist_id\", \"completion_date\"\n",
    "    )\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .partitionBy(\"completion_date\")\\\n",
    "    .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.matches_maps_bucketed\")\n",
    "\n",
    "\n",
    "#CREATE TABLE match_details_bucketed\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.match_details_bucketed\"\"\")\n",
    "bucketedDDL = \"\"\"\n",
    " CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "     match_id STRING,\n",
    "     player_gamertag STRING,\n",
    "     player_total_kills INTEGER,\n",
    "     player_total_deaths INTEGER\n",
    " )\n",
    " USING iceberg\n",
    " PARTITIONED BY (bucket(16, match_id));\n",
    " \"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "matchDetailsSelected.select(\n",
    "    \"match_id\", \"player_gamertag\", \"player_total_kills\", \"player_total_deaths\")\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.match_details_bucketed\")\n",
    "\n",
    "\n",
    "#JOIN TABLES TO CREATE DATAFRAME\n",
    "spark.sql(\n",
    "\"\"\"\n",
    "    SELECT \n",
    "         mmb.match_id,\n",
    "         mmb.mapid, \n",
    "         mmb.name as map_name, \n",
    "         mmb.is_team_game, \n",
    "         mmb.playlist_id, \n",
    "         mmb.completion_date,\n",
    "         mdb.player_gamertag, \n",
    "         mdb.player_total_kills, \n",
    "         mdb.player_total_deaths,\n",
    "         mmpb.medal_id, \n",
    "         mmpb.count as medals_count,\n",
    "         mmpb.classification as medal_classification\n",
    "    FROM bootcamp.matches_maps_bucketed AS mmb\n",
    "    JOIN bootcamp.match_details_bucketed AS mdb ON mdb.match_id=mmb.match_id\n",
    "    JOIN bootcamp.medals_matches_players_bucketed AS mmpb ON mmpb.match_id=mmb.match_id\n",
    "\"\"\"\n",
    ").createOrReplaceTempView(\"filteredDF\")\n",
    "\n",
    "##END OF TASK 3\n",
    "\n",
    "##TASK 4: Aggregate the joined data frame to figure out the following questions\n",
    "\n",
    "#Which player averages the most kills per game?\n",
    "players_avg_kills = spark.sql(\n",
    "\"\"\"\n",
    "    WITH deduplication as(\n",
    "    SELECT \n",
    "         match_id,\n",
    "         player_gamertag, \n",
    "         player_total_kills\n",
    "    FROM filteredDF\n",
    "    GROUP BY match_id,player_gamertag,player_total_kills)\n",
    "    SELECT \n",
    "        match_id,\n",
    "        player_gamertag,\n",
    "        sum(player_total_kills)/count(distinct match_id) as avg_kills_per_game\n",
    "    FROM deduplication\n",
    "    GROUP BY match_id,player_gamertag\n",
    "    ORDER BY sum(player_total_kills)/count(distinct match_id) desc\n",
    "\"\"\"\n",
    ")\n",
    "#Which playlist gets played the most?\n",
    "playlists = spark.sql(\n",
    "\"\"\"\n",
    "    WITH playlist_played as(\n",
    "    SELECT \n",
    "         match_id,\n",
    "         playlist_id, \n",
    "         count(1) as playlist_count\n",
    "    FROM filteredDF\n",
    "    GROUP BY match_id,playlist_id)\n",
    "    SELECT \n",
    "        playlist_id,\n",
    "        sum(playlist_count) num_plays\n",
    "    FROM playlist_played\n",
    "    GROUP BY playlist_id\n",
    "    ORDER BY sum(playlist_count) desc\n",
    "\"\"\"\n",
    ")\n",
    "#Which map gets played the most?\n",
    "maps_played = spark.sql(\n",
    "\"\"\"\n",
    "    WITH maps_played as(\n",
    "    SELECT \n",
    "         match_id,\n",
    "         mapid,\n",
    "         map_name,\n",
    "         count(1) as map_count\n",
    "    FROM filteredDF\n",
    "    GROUP BY match_id,mapid,map_name\n",
    "    )\n",
    "    SELECT \n",
    "        mapid,\n",
    "        map_name,\n",
    "        sum(map_count) num_plays\n",
    "    FROM maps_played\n",
    "    GROUP BY  mapid,map_name\n",
    "    ORDER BY sum(map_count) desc\n",
    "\"\"\"\n",
    ")\n",
    "#Which map do players get the most Killing Spree medals on?\n",
    "most_killing_spree = spark.sql(\n",
    "\"\"\"\n",
    "    WITH maps_and_medals as(\n",
    "    SELECT \n",
    "         mapid,\n",
    "         map_name,\n",
    "         medal_classification,\n",
    "         medals_count\n",
    "    FROM filteredDF\n",
    "    WHERE medal_classification='KillingSpree'\n",
    "    GROUP BY mapid,map_name,medal_classification,medals_count\n",
    "    )\n",
    "    SELECT \n",
    "         mapid,\n",
    "         map_name,\n",
    "         medal_classification,\n",
    "         sum(medals_count) num_medals\n",
    "    FROM maps_and_medals\n",
    "    GROUP BY  mapid,map_name,medal_classification\n",
    "    ORDER BY sum(medals_count) desc\n",
    "\"\"\"\n",
    ")\n",
    "##END OF TASK 4\n",
    "\n",
    "#TASK 5: Try different .sortWithinPartitions to see which has the smallest data size\n",
    "\n",
    "#SORT AGGREGATED DATAFRAME\n",
    "sorted_df=players_avg_kills.sortWithinPartitions(col(\"match_id\"))\n",
    "\n",
    "#CREATE SORTED TABLE\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.players_avg_kills_sorted\"\"\")\n",
    "sortedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.players_avg_kills_sorted (\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    avg_kills_per_game REAL\n",
    " )\n",
    " USING iceberg;\n",
    " \"\"\"\n",
    "spark.sql(sortedDDL)\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "sorted_df.select(\n",
    "    \"match_id\", \"player_gamertag\", \"avg_kills_per_game\")\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .saveAsTable(\"bootcamp.players_avg_kills_sorted\")\n",
    "\n",
    "#SORT AGGREGATED DATAFRAME\n",
    "sorted_df=playlists.sortWithinPartitions(col(\"playlist_id\"))\n",
    "#CREATE SORTED TABLE\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.playlists_sorted\"\"\")\n",
    "sortedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.playlists_sorted (\n",
    "    playlist_id STRING,\n",
    "    num_plays INTEGER\n",
    " )\n",
    " USING iceberg;\n",
    " \"\"\"\n",
    "spark.sql(sortedDDL)\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "sorted_df.select(\n",
    "    \"playlist_id\", \"num_plays\")\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .saveAsTable(\"bootcamp.playlists_sorted\")\n",
    "\n",
    "#SORT AGGREGATED DATAFRAME\n",
    "sorted_df=maps_played.sortWithinPartitions(col(\"mapid\"))\n",
    "#CREATE SORTED TABLE\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.maps_played_sorted\"\"\")\n",
    "sortedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.maps_played_sorted (\n",
    "    mapid STRING,\n",
    "    map_name STRING,\n",
    "    num_plays INTEGER\n",
    " )\n",
    " USING iceberg;\n",
    " \"\"\"\n",
    "spark.sql(sortedDDL)\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "sorted_df.select(\n",
    "    \"mapid\", \"map_name\", \"num_plays\")\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .saveAsTable(\"bootcamp.maps_played_sorted\")\n",
    "\n",
    "#SORT AGGREGATED DATAFRAME\n",
    "sorted_df=most_killing_spree.sortWithinPartitions(col(\"mapid\"))\n",
    "#CREATE SORTED TABLE\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.most_killing_spree_sorted\"\"\")\n",
    "sortedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.most_killing_spree_sorted (\n",
    "    mapid STRING,\n",
    "    map_name STRING,\n",
    "    medal_classification STRING,\n",
    "    num_medals INTEGER\n",
    " )\n",
    " USING iceberg;\n",
    " \"\"\"\n",
    "spark.sql(sortedDDL)\n",
    "#LOAD DATA INTO BUCKETED TABLE\n",
    "sorted_df.select(\n",
    "    \"mapid\", \"map_name\", \"medal_classification\", \"num_medals\")\\\n",
    "    .write.mode(\"append\")\\\n",
    "    .saveAsTable(\"bootcamp.most_killing_spree_sorted\")\n",
    "\n",
    "#COMPARE SORTED DATAFRAME SIZES\n",
    "spark.sql(\"\"\"\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, 'players_avg_kills_sorted' as DF  FROM bootcamp.players_avg_kills_sorted.files\n",
    "UNION ALL\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, 'playlists_sorted' as DF FROM bootcamp.playlists_sorted.files\n",
    "UNION ALL\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, 'maps_played_sorted' as DF FROM bootcamp.maps_played_sorted.files\n",
    "UNION ALL\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, 'most_killing_spree_sorted' as DF FROM bootcamp.most_killing_spree_sorted.files\n",
    "\"\"\").show()\n",
    "##END OF TASK 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
